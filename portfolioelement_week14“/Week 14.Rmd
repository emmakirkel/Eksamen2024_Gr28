---
title: 'Text mining, sentiment analysis, and visualization'
date: 'created on 22 November 2020 and updated `r format(Sys.time(), "%d %B, %Y")`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

# Note - Before lab:
# Attach tidytext and textdata packages
# Run: get_sentiments(lexicon = "nrc")
# Should be prompted to install lexicon - choose yes!
# Run: get_sentiments(lexicon = "afinn")
# Should be prompted to install lexicon - choose yes!

```

**Note:** for more text analysis, you can fork & work through Casey O’Hara and Jessica Couture’s eco-data-sci workshop (available here https://github.com/oharac/text_workshop)

### Get the IPCC report:
```{r get-document}
got_path <- here("data","got.pdf")
got_text <- pdf_text(got_path)
```

Some things to notice:

- How cool to extract text out of a PDF! Do you think it will work with any PDF?

Answer: Fist we had to acquire the PDF file from Casey O’Hara and Jessica Couture’s eco-data-sci workshop. We did this by using the code got_path <- here("data","got.pdf"), got_text <- pdf_text(got_path). 
This question has two sides. As long as the PDF only consists of text it will work just fine. But if the pdf consists of only pictures, it will not work.

- Each row is a page of the PDF (i.e., this is a vector of strings, one for each page)
- The pdf_text() function only sees text that is "selectable"

Example: Just want to get text from a single page (e.g. Page 9)? 
```{r single-page}
got_p37 <- got_text[37]
got_p37
```

See how that compares to the text in the PDF on Page 9. What has pdftools added and where?

Answer: Compared to the text in the PDF on page 9, we can see that "pdftools" has added a \n to signify a line break in the PDF on page 9. Therefore - for every line break, pdftools has added an \n to signify a line break.
By using code: got_p37 <- got_text[37], we could get a single page from GOT. 

From Jessica and Casey's text mining workshop: “pdf_text() returns a vector of strings, one for each page of the pdf. So we can mess with it in tidyverse style, let’s turn it into a dataframe, and keep track of the pages. Then we can use stringr::str_split() to break the pages up into individual lines. Each line of the pdf is concluded with a backslash-n, so split on this. We will also add a line number in addition to the page number."

### Some wrangling:

- Split up pages into separate lines (separated by `\n`) using `stringr::str_split()`
- Unnest into regular columns using `tidyr::unnest()`
- Remove leading/trailing white space with `stringr::str_trim()`

```{r split-lines}
got_df <- data.frame(got_text) %>% 
  mutate(text_full = str_split(got_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

# Why '\\n' instead of '\n'? Because some symbols (e.g. \, *) need to be called literally with a starting \ to escape the regular expression. For example, \\a for a string actually contains \a. So the string that represents the regular expression '\n' is actually '\\n'.
# Although, this time round, it is working for me with \n alone. Wonders never cease.

# More information: https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html

```

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

### Get the tokens (individual words) in tidy format

Use `tidytext::unnest_tokens()` (which pulls from the `tokenizer`) package, to split columns into tokens. We are interested in *words*, so that's the token we'll use:

```{r tokenize}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
got_tokens

# See how this differs from `ipcc_df`
# Each word has its own row!
```
Answer: So here we have accessed the entire GOT text. We did this by splitting up entire pages into separate lines using "mutate(text_full = str_split(got_text, pattern = '\n')) %>%". Then we had to unnest the text, so that it could be put into columns by using "unnest(text_full) %>%". To remove leading/trailing white space we then used mutate(text_full = str_trim(text_full)). 


Let's count the words!
```{r count-words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)
got_wc
```
Answer: After running the code; ```{r count-words}, got_wc <- got_tokens %>%,  count(word) %>% , arrange(-n), got_wc, we got a list of the most used words. These words included "the", "and", "to". We are not really interested in these particular words, which forces us to remove these stop words.

OK...so we notice that a whole bunch of things show up frequently that we might not be interested in ("a", "the", "and", etc.). These are called *stop words*. Let's remove them. 

### Remove stop words:

See `?stop_words` and `View(stop_words)`to look at documentation for stop words lexicons.

We will *remove* stop words using `tidyr::anti_join()`:
```{r stopwords}
got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)
View(stop_words)
```

Now check the counts again: 
```{r count-words2}
got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)
```

What if we want to get rid of all the numbers (non-text) in `ipcc_stop`?
```{r skip-numbers}
# This code will filter out numbers by asking:
# If you convert to as.numeric, is it NA (meaning those words)?
# If it IS NA (is.na), then keep it (so all words are kept)
# Anything that is converted to a number is removed

got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of IPCC report words (non-numeric)

See more: https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html

```{r wordcloud-prep}
# There are almost 2000 unique words 
length(unique(got_no_numeric$word))

# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
got_top100 <- got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

```{r wordcloud}
got_cloud <- ggplot(data = got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

got_cloud
```

That's underwhelming. Let's customize it a bit:
```{r wordcloud-pro}
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

Answer: To get this ggplot, we went through a bunch of code. 
Removing stop words: ```{r stopwords}, got_stop <- got_tokens %>%, anti_join(stop_words) %>%, select(-got_text), View(stop_words)
Removing numbers from the PDF: got_no_numeric <- got_stop %>%, filter(is.na(as.numeric(word)))
After removing the stop words and numbers from the PDF, there were still 2000 unique words - which is to many. Therefore we had to filter the PDF to only give us the 100 most frequent words: got_top100 <- got_no_numeric %>%, count(word) %>%, arrange(-n) %>%, head(100)
We then converted the most frequent words into a word cloud: got_cloud <- ggplot(data = got_top100, aes(label = word)) + geom_text_wordcloud() + theme_minimal() got_cloud
To customize the ggplot, we then used the following code: 
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
Result: Using Ggplot, we can deduce some of the most used words. By removing stop words, numbers and reducing the PDF to its most important words, we can conclude that words like lord, king, father and Jon is used the most.

Cool! And you can facet wrap (for different reports, for example) and update other aesthetics. See more here: https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html

### Sentiment analysis

First, check out the ‘sentiments’ lexicon. From Julia Silge and David Robinson (https://www.tidytextmining.com/sentiment.html):

“The three general-purpose lexicons are

  -  AFINN from Finn Årup Nielsen,
  -  bing from Bing Liu and collaborators, and
  -  nrc from Saif Mohammad and Peter Turney

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.  The bing lexicon categorizes words in a binary fashion into positive and negative categories. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.  All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon."

Let's explore the sentiment lexicons. "bing" is included, other lexicons ("afinn", "nrc", "loughran") you'll be prompted to download.

**WARNING:** These collections include very offensive words. I urge you to not look at them in class.

"afinn": Words ranked from -5 (very negative) to +5 (very positive)
```{r afinn}
get_sentiments(lexicon = "afinn")
# Note: may be prompted to download (yes)

# Let's look at the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

# Do not look at negative words in class. 
afinn_pos
```
Answer: Moving on to the sentiment analysis part of the assignment. For the first sentiment analysis, we using the first of three lexicons. The "afinn" lexicon assigns words with a score that runs between -5 and 5. -5 meaning negative and 5 meaning positive. In our case we are filtering the value to only show word scores of 3,4 and 5 - using the following code: afinn_pos <- get_sentiments("afinn") %>%, filter(value %in% c(3,4,5)).
Result: The "afinn" lexicon shows us that words like beloved (3), brilliant (4) and breathtaking (5) all have a high sentiment score as it is positive words. 

bing: binary, "positive" or "negative"
```{r bing}
get_sentiments(lexicon = "bing")
```
Answer: Moving on to the "bing" lexicon. This lexicon simply categorizes words in a binary fashion into positive and negative categories. To bring fourth this sentiment analysis, we are using the following code: get_sentiments(lexicon = "bing"). 
Result: Using this lexicon, we can see that words like abysmal have a negative tone and words like acclaim have a positive tone. 

nrc:https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm
Includes bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative. 

**Citation for NRC lexicon**: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

Now nrc:
```{r nrc}
get_sentiments(lexicon = "nrc")
```
Answer: The last sentiment lexicon is the "nrc" lexicon. This lexicon sorts words into different categories. These include: anger, anticipation, disgust, fear, joy, sadness, surprise, trust and positive / negative. We simply apply "get_sentiments(lexicon = "nrc")", to bring fourth this lexicon. 
Result: Using this sentiment analysis we can get a detailed look at the different word associations. This will help with our final analysis.

Let's do sentiment analysis on the IPCC text data using afinn, and nrc. 


### Sentiment analysis with afinn: 

First, bind words in `ipcc_stop` to `afinn` lexicon:
```{r bind-afinn}
got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))
```

Let's find some counts (by sentiment ranking):
```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col()
```
Answer: Using the "afinn" sentiment analysis on the Ipcc PDF (got_afinn <- got_stop %>%, inner_join(get_sentiments("afinn"))), and thereafter plotting the sentiment ranking ggplot, (data = got_afinn_hist, aes(x = value, y = n)) + geom_col()) we can see that the report is mostly negative. This is not surprising as the main theme is global warming. 


Investigate some of the words in a bit more depth:
```{r afinn-2}
# What are these '2' words?
got_afinn2 <- got_afinn %>% 
  filter(value == 2)
```

```{r afinn-2-more}
# Check the unique 2-score words:
unique(got_afinn2$word)

# Count & plot them
got_afinn2_n <- got_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = got_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()

# OK so what's the deal with confidence? And is it really "positive" in the emotion sense? 
```

Look back at the IPCC report, and search for "confidence." Is it typically associated with emotion, or something else? 
Answer: Using the following code to plot the unique 2-word score: ggplot(data = got_afinn2_n, aes(x = word, y = n)) + geom_col() + coord_flip()
In the IPCC report, the word; 'confidence' is more of a technical term and expresses the scientists confidence in the data. The word is therefore associated with something bad as it is global warming that is the theme. 

We learn something important from this example: Just using a sentiment lexicon to match words will not differentiate between different uses of the word...(ML can start figuring it out with context, but we won't do that here).

Or we can summarize sentiment for the report: 
```{r summarize-afinn}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```

The mean and median indicate *slightly* positive overall sentiments based on the AFINN lexicon. 

### NRC lexicon for sentiment analysis

We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the IPCC non-stopword text with the nrc lexicon: 

```{r bind-bing}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
```

Wait, won't that exclude some of the words in our text? YES! We should check which are excluded using `anti_join()`:

```{r check-exclusions}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(got_exclude)

# Count to find the most excluded:
got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)
```
Answer: Using the `anti_join()` code, we can exclude some of the words using the "nrc" lexicon. These words include names such as "Tyrion" and "Jon". But also words such as eyes and hands. These words probably have been excluded due to their inability to be put into one of eight categories. 

**Lesson: always check which words are EXCLUDED in sentiment analysis using a pre-built lexicon! **

Now find some counts: 
```{r count-bing}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()
```
Answer: After using "nrc" to exclude words, we can ggplot the sentiment analysis results using the following code: ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) + geom_col(). 
Result: Here we can conclude that there are overall more positive than negative associations in the book, but that these are almost equally matched and that the most commonly displayed feelings are trust, fear, disgust and sadness. A possible explanation for this is that the book is about betrayal and the breaking of trust.

Or count by sentiment *and* word, then facet:
```{r count-nrc}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
got_nrc_gg

# Save it
ggsave(plot = got_nrc_gg, 
       here("figures","got_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```

Wait, so "confidence" is showing up in NRC lexicon as "fear"? Let's check:
```{r nrc-lord}
lord <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "lord")

# Yep, check it out:
lord
```

## Big picture takeaway

There are serious limitations of sentiment analysis using existing lexicons, and you should **think really hard** about your findings and if a lexicon makes sense for your study. Otherwise, word counts and exploration alone can be useful! 

## Your task

Taking this script as a point of departure, apply sentiment analysis on the Game of Thrones. You will find a pdf in the data folder. What are the most common meaningful words and what emotions do you expect will dominate this volume? Are there any terms that are similarly ambiguous to the 'confidence' above? 

Answer: The Rmarkdown has shown us that the most common meaningful words include - mainly - character names such as lord, brother or father. We expected the emotions of sentiment of GOT to be mostly negative due to the reputation and story of the books. Although our research has shown us that the emotions are more dominated by both positive and negative emotions. For example - when finding the ambiguous word; lord - it shows us that the word holds both negative and positive meaning. 

### Credits: 
This tutorial is inspired by Allison Horst's Advanced Statistics and Data Analysis.